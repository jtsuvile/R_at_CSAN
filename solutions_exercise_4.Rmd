---
title: "Solutions for exercise set 4"
output: html_document
---g

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r eval=FALSE}
# Working on the emotions and pain data set from exercise 2
library(car)
library(apaTables)
library(tidyverse)
# Start by reading in the data
data <- read.csv('/Users/juusu53/Documents/projects/opetus/R_for_scientists/R_at_CSAN/data/exercise_data.csv')

# Let's do the same cleaning up we did in exercise set 2
data$sex <- factor(data$sex, levels=c(1,2), labels=c('female','male'))
data$education <- factor(data$education, levels=c(1,2,3,4), labels=c('primary', 'secondary', 'vocational', 'university'))

## 4.1 Simple statistical tests

# Q4.1.1
# checking extent of pain in the body for each sex
res <- t.test(data$curr_pain_extent ~ data$sex)
# see the results
res 

# Q4.1.2 
# t stands for test statistic, df stands for the degrees of freedom, p-value tells the p-valua
# you can call these from the results with 
res$statistic
res$parameter
res$p.value
# respectively. Similarly, for confidence interval
res$conf.int
# and means
res$estimate

# Q4.1.3
# for one-sided alternative hypothesis, you need to define which way you assume the 
# difference to be with either "less" or "greater"
t.test(data$curr_pain_extent ~ data$sex, alternative='greater')

# Q4.1.4
# built-in help tells us that var.equal parameter controls this
# if we want to specify that we assume equal variances, we can include
# var.equal = TRUE
t.test(data$curr_pain_extent ~ data$sex, var.equal = TRUE)

# 4.2 Testing assumptions

# Q4.2.1
# equality of variances
# our data are not normally distributed (as we learn below) so we'll use fligner-killeen's test
# which is very robust against departures from normality
fligner.test(data$curr_pain_extent ~ data$sex)

# Q4.2.2
# In the case of all of these tests, the null hypothesis (H0) is that 
# the two samples have equal variances (i.e. no difference in variances 
# between the groups), and H1 is that there is a difference. Therefore,
# if you get a high p-value, it's an indication of your samples having
# equal variances (keep H0), and if you get a low p-value, it's an indication
# of there being a difference and you having to reject H0.

# For the exercise data, we get 
# Fligner-Killeen:med chi-squared = 34.265, df = 1, p-value = 4.809e-09
# The p value is so small that we should reject the null hypothesis

# Q4.2.3

# Plotting tests for normality
hist(data$curr_pain_extent)
qqnorm(data$curr_pain_extent)
qqline(data$curr_pain_extent)
# Looks like the data have very heavy skew

# Q4.2.4
shapiro.test(data$curr_pain_extent)
# nope, definitely not normally distributed!

## 4.3 Regression
# first, let's only work with rows with no missing values (makes our life easier)
data_complete <- na.omit(data)
# I have decided to use the extent of current pain as the dependent variable. However, it's coded as 0-1 (proportion of body in pain), let's change that to show percentages (0-100) instead to get more informative betas
data_complete$curr_pain_percent <- data_complete$curr_pain_extent*100

# Q4.3.1 
# simple regression 
lm(curr_pain_percent ~ work_physical, data=data_complete)

# Q4.3.2 
model1 <- lm(curr_pain_percent ~ work_physical, data=data_complete)
summary(model1)

# Q4.3.3 
# a slightly more complex linear model
model2 <- lm(curr_pain_percent ~ work_sitting + work_physical, data=data_complete)
summary(model2)

# Q4.3.4 
# Comparing the two models 
anova(model1, model2)

# It looks like in this case the more complex model is better than the simpler one (F = 4.57, p = 0.033), so we should keep the information about proportion of work time spent sitting in the model even though it is not as strong of a predictor as proportion of time spent in physically taxing work

# Q4.3.5 
# Output outcome into a pretty table
apa.reg.table(model2)

## Bonus! Logistic regression

# Let's create a binary variable about whether subject is likely to have or not have depression in this sample, based on their self rated depression (NB: this is not a clinically sound way to do this, this is just for demonstration purposes!)

# let's do the splitting with tidyverse and save the result to a new data frame
data_with_depression <- data_complete %>% 
  filter(depression >= 7 | depression <= 1) %>% # only select subs with very high or very low depression rating
  mutate(is_depressed = depression >= 7) # create a new logical column is_depressed based on the depression rating

# Now, we can run a logistic regression where dependent variable is the one we just created
bin1 <- glm(formula = is_depressed ~ joy + pain + anxiety, 
    data=data_with_depression,
    family = 'binomial')

# check to see outcome
summary(bin1)

# we can also use the gtsummary package to get the logistic regression stats to a nice table
library(gtsummary)
tbl_regression(bin1, exponentiate = TRUE)
# the results are shown in viewer (in the same quadrant your plots are shown in)

# NB: This package seems to only work if you use glm() function to create your model. If you wanted to use gtsummary to display the results of the linear models, you would need to tweak your code for the linear regression to use glm() instead of lm() 

```
